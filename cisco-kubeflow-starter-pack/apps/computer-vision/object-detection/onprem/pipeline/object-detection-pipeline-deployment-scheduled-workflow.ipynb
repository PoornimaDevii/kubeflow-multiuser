{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection Pipeline Scheduled Workflow on UCS using Darknet & YOLO\n",
    "\n",
    "This notebook focuses on implementing object detection scheduled workflow as a Kubeflow pipeline on Cisco UCS by using Darknet which is a open-source neural network framework, YOLO (You Only Look Once) which is a real-time object detection system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone Cisco Kubeflow starter pack repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'cisco-kubeflow-starter-pack'...\n",
      "remote: Enumerating objects: 612, done.\u001b[K\n",
      "remote: Counting objects: 100% (612/612), done.\u001b[K\n",
      "remote: Compressing objects: 100% (342/342), done.\u001b[K\n",
      "remote: Total 6640 (delta 291), reused 439 (delta 139), pack-reused 6028\u001b[K\n",
      "Receiving objects: 100% (6640/6640), 45.19 MiB | 48.11 MiB/s, done.\n",
      "Resolving deltas: 100% (2672/2672), done.\n"
     ]
    }
   ],
   "source": [
    "BRANCH_NAME=\"dev\" #Provide git branch \"master\" or \"dev\"\n",
    "! git clone -b $BRANCH_NAME https://github.com/CiscoAI/cisco-kubeflow-starter-pack.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kfp==1.0.1 in ./.local/lib/python3.6/site-packages (1.0.1)\n",
      "Requirement already satisfied: pillow==7.2.0 in ./.local/lib/python3.6/site-packages (7.2.0)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from kfp==1.0.1) (1.2.2)\n",
      "Requirement already satisfied: jsonschema>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from kfp==1.0.1) (3.2.0)\n",
      "Requirement already satisfied: Deprecated in ./.local/lib/python3.6/site-packages (from kfp==1.0.1) (1.2.10)\n",
      "Requirement already satisfied: tabulate in ./.local/lib/python3.6/site-packages (from kfp==1.0.1) (0.8.7)\n",
      "Requirement already satisfied: requests-toolbelt>=0.8.0 in ./.local/lib/python3.6/site-packages (from kfp==1.0.1) (0.9.1)\n",
      "Requirement already satisfied: click in ./.local/lib/python3.6/site-packages (from kfp==1.0.1) (7.1.2)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from kfp==1.0.1) (5.3)\n",
      "Requirement already satisfied: google-auth>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from kfp==1.0.1) (1.11.0)\n",
      "Requirement already satisfied: kfp-server-api<2.0.0,>=0.2.5 in ./.local/lib/python3.6/site-packages (from kfp==1.0.1) (1.0.4)\n",
      "Requirement already satisfied: kubernetes<12.0.0,>=8.0.0 in /usr/local/lib/python3.6/dist-packages (from kfp==1.0.1) (10.0.1)\n",
      "Requirement already satisfied: strip-hints in ./.local/lib/python3.6/site-packages (from kfp==1.0.1) (0.1.9)\n",
      "Requirement already satisfied: google-cloud-storage>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from kfp==1.0.1) (1.25.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp==1.0.1) (0.15.7)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp==1.0.1) (45.1.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp==1.0.1) (1.4.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /usr/lib/python3/dist-packages (from jsonschema>=3.0.1->kfp==1.0.1) (1.11.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp==1.0.1) (19.3.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from Deprecated->kfp==1.0.1) (1.11.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from requests-toolbelt>=0.8.0->kfp==1.0.1) (2.22.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp==1.0.1) (4.0.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp==1.0.1) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp==1.0.1) (0.2.8)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=0.2.5->kfp==1.0.1) (2.8.1)\n",
      "Requirement already satisfied: urllib3>=1.15 in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=0.2.5->kfp==1.0.1) (1.25.8)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kfp-server-api<2.0.0,>=0.2.5->kfp==1.0.1) (2019.11.28)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp==1.0.1) (0.57.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp==1.0.1) (1.3.0)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from strip-hints->kfp==1.0.1) (0.30.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage>=1.13.0->kfp==1.0.1) (1.3.0)\n",
      "Requirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage>=1.13.0->kfp==1.0.1) (0.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema>=3.0.1->kfp==1.0.1) (2.1.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.1->requests-toolbelt>=0.8.0->kfp==1.0.1) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.0.1->requests-toolbelt>=0.8.0->kfp==1.0.1) (2.6)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth>=1.6.1->kfp==1.0.1) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib->kubernetes<12.0.0,>=8.0.0->kfp==1.0.1) (3.1.0)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==1.0.1) (1.16.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==1.0.1) (1.51.0)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==1.0.1) (2019.3)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==1.0.1) (3.11.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install kfp==1.0.1 pillow==7.2.0 --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display_html\n",
    "display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import yaml\n",
    "import calendar\n",
    "import requests\n",
    "import logging\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import tarfile, zipfile\n",
    "\n",
    "#Kubeflow\n",
    "import kfp\n",
    "import kfp_server_api \n",
    "from kfp_server_api import models, api, ApiPipelineSpec, ApiTrigger, ApiPeriodicSchedule\n",
    "from kfp.aws import use_aws_secret\n",
    "import kfp.compiler as compiler\n",
    "\n",
    "#Kubernetes\n",
    "from kubernetes import client\n",
    "\n",
    "#Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pipeline components\n",
    "\n",
    "Declare the paths of respective YAML configuration files of each of the pipeline components, in order to load each component into a variable for pipeline execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='cisco-kubeflow-starter-pack/apps/computer-vision/object-detection/onprem/pipeline/components/v2/'\n",
    "component_root_dwn= path+'download/'\n",
    "component_root_katib= path+'katib/'\n",
    "component_root_train= path+'train/'\n",
    "component_root_validate= path+'validate/'\n",
    "component_root_convert_ncnn=path+'conversion_ncnn/'\n",
    "component_root_cleanup=path+'cleanup/'\n",
    "\n",
    "download_op = kfp.components.load_component_from_file(os.path.join(component_root_dwn, 'component.yaml'))\n",
    "hptuning_op = kfp.components.load_component_from_file(os.path.join(component_root_katib, 'component.yaml'))\n",
    "train_op = kfp.components.load_component_from_file(os.path.join(component_root_train, 'component.yaml'))\n",
    "validate_op = kfp.components.load_component_from_file(os.path.join(component_root_validate, 'component.yaml'))\n",
    "convert_ncnn_op=kfp.components.load_component_from_file(os.path.join(component_root_convert_ncnn, 'component.yaml'))\n",
    "cleanup_op=kfp.components.load_component_from_file(os.path.join(component_root_cleanup, 'component.yaml'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define volume claim & volume mount for storage during pipeline execution\n",
    "\n",
    "Persistent volume claim & volume mount is created for the purpose of storing entities such as Dataset, model files, etc, and to share the stored resources between the various components of the pipeline during it's execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfs_pvc = client.V1PersistentVolumeClaimVolumeSource(claim_name='nfs')\n",
    "nfs_volume = client.V1Volume(name='nfs', persistent_volume_claim=nfs_pvc)\n",
    "nfs_volume_mount = client.V1VolumeMount(mount_path='/mnt/', name='nfs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define pipeline function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus=1 # Number of GPUs to run training\n",
    "\n",
    "def object_detection_pipeline(\n",
    "    nfs_path='/mnt/object_detection',\n",
    "    s3_path=\"s3://object-det-test/001\",    # AWS S3 bucket URL. Ex: s3://<bucket-name>/ \n",
    "    namespace='kubeflow',               # Namespace on which trained model is to be deployed for prediction\n",
    "    timestamp=\"123456\",                 # Current timestamp\n",
    "    cfg_data=\"voc.data\",                # Config file containing file name specifications of train, test and validate datasets\n",
    "    cfg_file=\"yolov3-voc.cfg\",          # Config file containing hyperparameters declarations Ex: yolov3.cfg / yolov4.cfg\n",
    "    weights=\"yolov3-voc_50000.weights\", # Weights which are already pre-trained upto 50000 iterations is used. Therefore,  \n",
    "                                        # training happens from 50000 iterations upto a limit of max_batches (say 50200) specified \n",
    "                                        # in cfg_file. \n",
    "    trials=1,                           # Total number of trials under Katib experiment\n",
    "    gpus_per_trial=1,                   # Maximum GPUS to be used for each trial\n",
    "    classes_file=\"voc.names\"            # File containing the names of object classes (such as person, bus, car,etc)\n",
    "):\n",
    "#     # Download component\n",
    "    dwn_task = download_op(s3_path=s3_path,timestamp=timestamp,\n",
    "                           cfg_data=cfg_data\n",
    "                          ).apply(use_aws_secret(secret_name='aws-secret', aws_access_key_id_name='AWS_ACCESS_KEY_ID', aws_secret_access_key_name='AWS_SECRET_ACCESS_KEY'))\n",
    "    dwn_task.add_volume(nfs_volume)\n",
    "    dwn_task.add_volume_mount(nfs_volume_mount) \n",
    "    \n",
    "#     # HP tuning (Katib) component\n",
    "    hptuning_task = hptuning_op(cfg_data=cfg_data,             # Config file containing file name specifications of train, test and validate datasets\n",
    "                                cfg_file=cfg_file,             # Config file containing hyperparameters declarations Ex: yolov3.cfg / yolov4.cfg\n",
    "                                weights=weights,               # Pre-trained weights for VOC dataset\n",
    "                                trials=trials,                 # total number of trials under Katib experiment\n",
    "                                timestamp=timestamp,           # Current timestamp to create unique experiment \n",
    "                                                               # Ex: object-detection-1599547688-random-588d7877f5-zvlx5\n",
    "                                gpus_per_trial=gpus_per_trial, # Maximum GPUS to be used for each trial \n",
    "                                )\n",
    "    hptuning_task.add_volume(nfs_volume)\n",
    "    hptuning_task.add_volume_mount(nfs_volume_mount)\n",
    "    hptuning_task.after(dwn_task)\n",
    "    \n",
    "    # Train component\n",
    "    train_task = train_op(cfg_data=cfg_data,          # Config file containing file name specifications of train, test and validate datasets\n",
    "                          cfg_file=cfg_file,          # Config file containing hyperparameters declarations Ex: yolov3.cfg / yolov4.cfg\n",
    "                          weights=weights,            # Pre-trained weights for VOC dataset\n",
    "                          gpus=gpus,             \n",
    "                          timestamp=timestamp\n",
    "                         )\n",
    "    train_task.add_volume(nfs_volume)\n",
    "    train_task.add_volume_mount(nfs_volume_mount).set_gpu_limit(gpus)  #Maximum GPUs to be used for training\n",
    "    train_task.after(hptuning_task)\n",
    "    \n",
    "    # Validation component\n",
    "    validate_task = validate_op(nfs_path=nfs_path,\n",
    "                                s3_path=s3_path,\n",
    "                                cfg_data=cfg_data,          # Config file containing file name specifications of train, test and validate datasets\n",
    "                                cfg_file=cfg_file,          # Config file containing hyperparameters declarations Ex: yolov3.cfg / yolov4.cfg\n",
    "                                weights=weights,            # Pre-trained weights for VOC dataset\n",
    "                                timestamp=timestamp\n",
    "                                ).apply(use_aws_secret(secret_name='aws-secret', aws_access_key_id_name='AWS_ACCESS_KEY_ID', aws_secret_access_key_name='AWS_SECRET_ACCESS_KEY'))\n",
    "    validate_task.add_volume(nfs_volume)\n",
    "    validate_task.add_volume_mount(nfs_volume_mount)\n",
    "    validate_task.after(train_task)\n",
    "    \n",
    "    # Darknet to ncnn conversion component\n",
    "    conversion_ncnn_task=convert_ncnn_op(push_to_s3=\"true\",  # Flag to decide whether to upload the trained weights and converted\n",
    "                                                    # model to S3 bucket for future inferencing on anyother environment or\n",
    "                                                    # proceeding to serve on UCS (Input: true/false)\n",
    "                               s3_path=s3_path,\n",
    "                               timestamp=timestamp,\n",
    "                               ).apply(use_aws_secret(secret_name='aws-secret', aws_access_key_id_name='AWS_ACCESS_KEY_ID', aws_secret_access_key_name='AWS_SECRET_ACCESS_KEY'))\n",
    "    conversion_ncnn_task.add_volume(nfs_volume)\n",
    "    conversion_ncnn_task.add_volume_mount(nfs_volume_mount)\n",
    "    conversion_ncnn_task.after(validate_task)\n",
    "    \n",
    "     # Clean up component\n",
    "    cleanup_task = cleanup_op(nfs_path=nfs_path,\n",
    "                                timestamp=timestamp\n",
    "                                ).apply(use_aws_secret(secret_name='aws-secret', aws_access_key_id_name='AWS_ACCESS_KEY_ID', aws_secret_access_key_name='AWS_SECRET_ACCESS_KEY'))\n",
    "    cleanup_task.add_volume(nfs_volume)\n",
    "    cleanup_task.add_volume_mount(nfs_volume_mount)\n",
    "    cleanup_task.after(conversion_ncnn_task)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile pipeline function\n",
    "\n",
    "Compile the pipeline function to create a tar ball for the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.6/site-packages/kfp/components/_data_passing.py:168: UserWarning: Missing type name was inferred as \"Integer\" based on the value \"1\".\n",
      "  warnings.warn('Missing type name was inferred as \"{}\" based on the value \"{}\".'.format(type_name, str(value)))\n"
     ]
    }
   ],
   "source": [
    "# Compile pipeline\n",
    "try:\n",
    "    compiler.Compiler().compile(object_detection_pipeline, 'object-detection-schedule.tar.gz')\n",
    "except RuntimeError as err:\n",
    "    logging.debug(err)\n",
    "    logging.info(\"Argo workflow failed validation check but it can still be used to run experiments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition to extract YAML from object-detection-schedule.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_pipeline_yaml(package_file):\n",
    "    def _choose_pipeline_yaml_file(file_list) -> str:\n",
    "      yaml_files = [file for file in file_list if file.endswith('.yaml')]\n",
    "      if len(yaml_files) == 0:\n",
    "        raise ValueError('Invalid package. Missing pipeline yaml file in the package.')\n",
    "\n",
    "      if 'pipeline.yaml' in yaml_files:\n",
    "        return 'pipeline.yaml'\n",
    "      else:\n",
    "        if len(yaml_files) == 1:\n",
    "          return yaml_files[0]\n",
    "        raise ValueError('Invalid package. There is no pipeline.yaml file and there are multiple yaml files.')\n",
    "\n",
    "    if package_file.endswith('.tar.gz') or package_file.endswith('.tgz'):\n",
    "      with tarfile.open(package_file, \"r:gz\") as tar:\n",
    "        file_names = [member.name for member in tar if member.isfile()]\n",
    "        pipeline_yaml_file = _choose_pipeline_yaml_file(file_names)\n",
    "        with tar.extractfile(tar.getmember(pipeline_yaml_file)) as f:\n",
    "          return yaml.safe_load(f)\n",
    "    elif package_file.endswith('.zip'):\n",
    "      with zipfile.ZipFile(package_file, 'r') as zip:\n",
    "        pipeline_yaml_file = _choose_pipeline_yaml_file(zip.namelist())\n",
    "        with zip.open(pipeline_yaml_file) as f:\n",
    "          return yaml.safe_load(f)\n",
    "    elif package_file.endswith('.yaml') or package_file.endswith('.yml'):\n",
    "      with open(package_file, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "    else:\n",
    "      raise ValueError('The package_file '+ package_file + ' should ends with one of the following formats: [.tar.gz, .tgz, .zip, .yaml, .yml]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract YAML from tar.gz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_package_path=\"object-detection-schedule.tar.gz\"\n",
    "pipeline_obj = _extract_pipeline_yaml(pipeline_package_path)\n",
    "pipeline_json_string = json.dumps(pipeline_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp_client = kfp.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define periodic schedule of run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = kfp_server_api.models.api_job.ApiJob(\n",
    "            name = \"object-detection-periodic-schedule\",\n",
    "            max_concurrency =\"1\",\n",
    "            enabled = True,\n",
    "            pipeline_spec = kfp_server_api.models.ApiPipelineSpec(\n",
    "                workflow_manifest = pipeline_json_string,\n",
    "            ),\n",
    "            trigger = kfp_server_api.models.ApiTrigger(\n",
    "                periodic_schedule = kfp_server_api.models.ApiPeriodicSchedule(\n",
    "                    interval_second = \"3600\"\n",
    "            ),\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a scheduled job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_job = kfp_client.jobs.create_job(body=job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'82602d5d-6e3d-4521-805e-43474221f4b2'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op_job.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup scheduled job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduled job with job ID 76adfcd7-fa50-4206-9aae-1a21887b4546 has been deleted\n"
     ]
    }
   ],
   "source": [
    "kfp_client.jobs.delete_job(id=op_job.id)\n",
    "print('Scheduled job with job ID %s has been deleted' %op_job.id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
