{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection Pipeline on UCS using Darknet & YOLO\n",
    "\n",
    "This notebook focuses on implementing object detection as a Kubeflow pipeline on Cisco UCS by using Darknet which is a open-source neural network framework, YOLO (You Only Look Once) which is a real-time object detection system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone Cisco Kubeflow starter pack repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'cisco-kubeflow-starter-pack'...\n",
      "remote: Enumerating objects: 267, done.\u001b[K\n",
      "remote: Counting objects: 100% (267/267), done.\u001b[K\n",
      "remote: Compressing objects: 100% (130/130), done.\u001b[K\n",
      "remote: Total 7239 (delta 131), reused 191 (delta 84), pack-reused 6972\u001b[K\n",
      "Receiving objects: 100% (7239/7239), 46.41 MiB | 32.16 MiB/s, done.\n",
      "Resolving deltas: 100% (2995/2995), done.\n"
     ]
    }
   ],
   "source": [
    "BRANCH_NAME=\"dev\" #Provide git branch \"master\" or \"dev\"\n",
    "! git clone -b $BRANCH_NAME https://github.com/CiscoAI/cisco-kubeflow-starter-pack.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kfp==1.0.1\n",
      "  Downloading kfp-1.0.1.tar.gz (116 kB)\n",
      "\u001b[K     |████████████████████████████████| 116 kB 19.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pillow==7.2.0\n",
      "  Downloading Pillow-7.2.0-cp36-cp36m-manylinux1_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 28.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mlflow==1.13.1\n",
      "  Downloading mlflow-1.13.1-py3-none-any.whl (14.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.1 MB 23.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from kfp==1.0.1) (5.3)\n",
      "Requirement already satisfied: google-cloud-storage>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from kfp==1.0.1) (1.25.0)\n",
      "Requirement already satisfied: kubernetes<12.0.0,>=8.0.0 in /usr/local/lib/python3.6/dist-packages (from kfp==1.0.1) (10.0.1)\n",
      "Requirement already satisfied: google-auth>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from kfp==1.0.1) (1.11.0)\n",
      "Collecting requests_toolbelt>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 519 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from kfp==1.0.1) (1.2.2)\n",
      "Collecting kfp-server-api<2.0.0,>=0.2.5\n",
      "  Downloading kfp-server-api-1.3.0.tar.gz (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 941 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jsonschema>=3.0.1 in /usr/local/lib/python3.6/dist-packages (from kfp==1.0.1) (3.2.0)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.8.7-py3-none-any.whl (24 kB)\n",
      "Collecting click\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 277 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting Deprecated\n",
      "  Downloading Deprecated-1.2.11-py2.py3-none-any.whl (9.1 kB)\n",
      "Collecting strip-hints\n",
      "  Downloading strip-hints-0.1.9.tar.gz (30 kB)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from mlflow==1.13.1) (3.11.2)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl (9.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.5 MB 108.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Flask\n",
      "  Downloading Flask-1.1.2-py2.py3-none-any.whl (94 kB)\n",
      "\u001b[K     |████████████████████████████████| 94 kB 809 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from mlflow==1.13.1) (1.11.0)\n",
      "Collecting gitpython>=2.1.0\n",
      "  Downloading GitPython-3.1.13-py3-none-any.whl (159 kB)\n",
      "\u001b[K     |████████████████████████████████| 159 kB 42.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from mlflow==1.13.1) (0.3)\n",
      "Collecting prometheus-flask-exporter\n",
      "  Downloading prometheus_flask_exporter-0.18.1.tar.gz (21 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mlflow==1.13.1) (1.18.1)\n",
      "Collecting azure-storage-blob>=12.0.0\n",
      "  Downloading azure_storage_blob-12.7.1-py2.py3-none-any.whl (339 kB)\n",
      "\u001b[K     |████████████████████████████████| 339 kB 108.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gunicorn; platform_system != \"Windows\"\n",
      "  Downloading gunicorn-20.0.4-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 1.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.17.3 in /usr/local/lib/python3.6/dist-packages (from mlflow==1.13.1) (2.22.0)\n",
      "Collecting alembic<=1.4.1\n",
      "  Downloading alembic-1.4.1.tar.gz (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 35.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: docker>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from mlflow==1.13.1) (4.1.0)\n",
      "Collecting sqlalchemy\n",
      "  Downloading SQLAlchemy-1.3.23-cp36-cp36m-manylinux2010_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 37.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sqlparse>=0.3.1\n",
      "  Downloading sqlparse-0.4.1-py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 137 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting databricks-cli>=0.8.7\n",
      "  Downloading databricks-cli-0.14.1.tar.gz (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 634 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from mlflow==1.13.1) (2.8.1)\n",
      "Collecting querystring-parser\n",
      "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
      "Requirement already satisfied: google-resumable-media<0.6dev,>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage>=1.13.0->kfp==1.0.1) (0.5.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage>=1.13.0->kfp==1.0.1) (1.3.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp==1.0.1) (2019.11.28)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp==1.0.1) (1.25.8)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp==1.0.1) (45.1.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp==1.0.1) (0.57.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.6/dist-packages (from kubernetes<12.0.0,>=8.0.0->kfp==1.0.1) (1.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp==1.0.1) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp==1.0.1) (4.0.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.6.1->kfp==1.0.1) (4.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp==1.0.1) (0.15.7)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp==1.0.1) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema>=3.0.1->kfp==1.0.1) (19.3.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from Deprecated->kfp==1.0.1) (1.11.2)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from strip-hints->kfp==1.0.1) (0.30.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->mlflow==1.13.1) (2019.3)\n",
      "Collecting itsdangerous>=0.24\n",
      "  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask->mlflow==1.13.1) (0.16.1)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask->mlflow==1.13.1) (2.11.0)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.5-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 466 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: prometheus_client in /usr/local/lib/python3.6/dist-packages (from prometheus-flask-exporter->mlflow==1.13.1) (0.7.1)\n",
      "Collecting azure-core<2.0.0,>=1.10.0\n",
      "  Downloading azure_core-1.11.0-py2.py3-none-any.whl (127 kB)\n",
      "\u001b[K     |████████████████████████████████| 127 kB 96.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cryptography>=2.1.4 in /usr/lib/python3/dist-packages (from azure-storage-blob>=12.0.0->mlflow==1.13.1) (2.1.4)\n",
      "Collecting msrest>=0.6.18\n",
      "  Downloading msrest-0.6.21-py2.py3-none-any.whl (85 kB)\n",
      "\u001b[K     |████████████████████████████████| 85 kB 970 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.17.3->mlflow==1.13.1) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.17.3->mlflow==1.13.1) (3.0.4)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.1.4.tar.gz (479 kB)\n",
      "\u001b[K     |████████████████████████████████| 479 kB 44.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting python-editor>=0.3\n",
      "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==1.0.1) (1.16.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib->kubernetes<12.0.0,>=8.0.0->kfp==1.0.1) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp==1.0.1) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema>=3.0.1->kfp==1.0.1) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask->mlflow==1.13.1) (1.1.1)\n",
      "Collecting smmap<4,>=3.0.1\n",
      "  Downloading smmap-3.0.5-py2.py3-none-any.whl (25 kB)\n",
      "Collecting isodate>=0.6.0\n",
      "  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 827 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp==1.0.1) (1.51.0)\n",
      "Building wheels for collected packages: kfp, kfp-server-api, strip-hints, prometheus-flask-exporter, alembic, databricks-cli, Mako\n",
      "  Building wheel for kfp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp: filename=kfp-1.0.1-py3-none-any.whl size=160933 sha256=2d4e5771a813747b38dce69d4d6c52b00b00996dd89e741375171e5af7f719cf\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/fd/88/f8/c59eb844a712adafc79d427cb057b1994c6b3d227893c554a8\n",
      "  Building wheel for kfp-server-api (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp-server-api: filename=kfp_server_api-1.3.0-py3-none-any.whl size=108934 sha256=fa54efbd11062a126ac9a49c7dec254eb6ff21c94588c6f0f481f11d953c1e12\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/f4/33/6a/c3e0fe65faf568ad6cc66260f187f97dd0e86b33145305c62c\n",
      "  Building wheel for strip-hints (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for strip-hints: filename=strip_hints-0.1.9-py2.py3-none-any.whl size=24671 sha256=dfa0d83ec2578261c84ca0ca134f2ffafd99ccb0ccbc53968f97a9bf21448552\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/21/6d/fa/7ed7c0560e1ef39ebabd5cc0241e7fca711660bae1ad752e2b\n",
      "  Building wheel for prometheus-flask-exporter (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for prometheus-flask-exporter: filename=prometheus_flask_exporter-0.18.1-py3-none-any.whl size=22573 sha256=fcdda86e20868a132281661c40bb2b205a48ec292e3f872852d673ab1547f316\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/fe/70/a9/22af6c68f513e58533fb7fd649f4cc5e2a27c24422a41a1bfa\n",
      "  Building wheel for alembic (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for alembic: filename=alembic-1.4.1-py2.py3-none-any.whl size=160574 sha256=67b6a72982e749816c12d81757da85cfd5c7444951fce022601b96d143b18ab6\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/e9/7b/aa/e18c983d8236b141f85838ba0f8e4e4ae9bcf7f1e00ff726ec\n",
      "  Building wheel for databricks-cli (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for databricks-cli: filename=databricks_cli-0.14.1-py3-none-any.whl size=102720 sha256=8df18919340409f3522f9978d07c3438564320ea2a59f24a404749fa3c206854\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/36/2c/2e/09bcfa0bdb7005b96213ff0967f9ab2697b8d07196d1edeeeb\n",
      "  Building wheel for Mako (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for Mako: filename=Mako-1.1.4-py2.py3-none-any.whl size=76438 sha256=435ccc226cf1b9817bf4eaa5a791427509d64a43c63d566cda953acb2100602c\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/3c/ee/c2/9651c6b977f9d2a1bb766970d190f71213e2ca47b36d8dc488\n",
      "Successfully built kfp kfp-server-api strip-hints prometheus-flask-exporter alembic databricks-cli Mako\n",
      "Installing collected packages: requests-toolbelt, kfp-server-api, tabulate, click, Deprecated, strip-hints, kfp, pillow, pandas, itsdangerous, Flask, smmap, gitdb, gitpython, prometheus-flask-exporter, azure-core, isodate, msrest, azure-storage-blob, gunicorn, sqlalchemy, Mako, python-editor, alembic, sqlparse, databricks-cli, querystring-parser, mlflow\n",
      "\u001b[33m  WARNING: The script tabulate is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script strip-hints is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts dsl-compile and kfp are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script flask is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script gunicorn is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script mako-render is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script alembic is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script sqlformat is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts databricks and dbfs are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script mlflow is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed Deprecated-1.2.11 Flask-1.1.2 Mako-1.1.4 alembic-1.4.1 azure-core-1.11.0 azure-storage-blob-12.7.1 click-7.1.2 databricks-cli-0.14.1 gitdb-4.0.5 gitpython-3.1.13 gunicorn-20.0.4 isodate-0.6.0 itsdangerous-1.1.0 kfp-1.0.1 kfp-server-api-1.3.0 mlflow-1.13.1 msrest-0.6.21 pandas-1.1.5 pillow-7.2.0 prometheus-flask-exporter-0.18.1 python-editor-1.0.4 querystring-parser-1.2.4 requests-toolbelt-0.9.1 smmap-3.0.5 sqlalchemy-1.3.23 sqlparse-0.4.1 strip-hints-0.1.9 tabulate-0.8.7\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install kfp==1.0.1 pillow==7.2.0 mlflow==1.13.1 --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display_html\n",
    "display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import yaml\n",
    "import calendar\n",
    "import requests\n",
    "import logging\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "#Kubeflow\n",
    "import kfp\n",
    "from kfp.aws import use_aws_secret\n",
    "import kfp.compiler as compiler\n",
    "\n",
    "#Kubernetes\n",
    "from kubernetes import client\n",
    "\n",
    "#Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "#MLFlow\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pipeline components\n",
    "\n",
    "Declare the paths of respective YAML configuration files of each of the pipeline components, in order to load each component into a variable for pipeline execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='cisco-kubeflow-starter-pack/apps/computer-vision/object-detection/onprem/pipeline/components/v2/'\n",
    "component_root_dwn= path+'download/'\n",
    "component_root_katib= path+'katib/'\n",
    "component_root_train= path+'train/'\n",
    "component_root_validate= path+'validate/'\n",
    "component_root_cleanup=path+'cleanup/'\n",
    "component_root_convert_ncnn=path+'conversion_ncnn/'\n",
    "\n",
    "download_op = kfp.components.load_component_from_file(os.path.join(component_root_dwn, 'component.yaml'))\n",
    "hptuning_op = kfp.components.load_component_from_file(os.path.join(component_root_katib, 'component.yaml'))\n",
    "train_op = kfp.components.load_component_from_file(os.path.join(component_root_train, 'component.yaml'))\n",
    "validate_op = kfp.components.load_component_from_file(os.path.join(component_root_validate, 'component.yaml'))\n",
    "convert_ncnn_op=kfp.components.load_component_from_file(os.path.join(component_root_convert_ncnn, 'component.yaml'))\n",
    "cleanup_op=kfp.components.load_component_from_file(os.path.join(component_root_cleanup, 'component.yaml'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define volume claim & volume mount for storage during pipeline execution\n",
    "\n",
    "Persistent volume claim & volume mount is created for the purpose of storing entities such as dataset, model files, etc, and to share the stored resources between the various components of the pipeline during it's execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfs_pvc = client.V1PersistentVolumeClaimVolumeSource(claim_name='nfs')\n",
    "nfs_volume = client.V1Volume(name='nfs', persistent_volume_claim=nfs_pvc)\n",
    "nfs_volume_mount = client.V1VolumeMount(mount_path='/mnt/', name='nfs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Katib hyperparameters configuration\n",
    "\n",
    "Specify your custom configuration of hyperparameters in dict format as shown below. \n",
    "If your configuration is present in YAML format, please use [this site](https://codebeautify.org/yaml-to-json-xml-csv) to convert it into JSON/dict format.\n",
    "\n",
    "Use [YAML sample](cisco-kubeflow-starter-pack/apps/computer-vision/object-detection/onprem/pipeline/sample_tuning_spec.yaml) of partial tuning spec for reference.\n",
    "\n",
    "In case of vice-versa conversion (from JSON to YAML), please utilize [this site](https://codebeautify.org/json-to-yaml). This can be used in case of manual modification of ```trialTemplate``` part present in Katib component's source code.\n",
    "\n",
    "\n",
    "#### Note: \n",
    "\n",
    "- Please provide only that part of configuration related to objective, algorithm, trial count related parameters & hyperparameters of Katib Experiment spec as shown below.\n",
    "- Even if you specify maxTrialCount in your configuration dict, the value will be overridden with 'trials' variable's value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_spec={\n",
    "   \"objective\":{\n",
    "      \"type\":\"minimize\",\n",
    "      \"goal\":0.4,\n",
    "      \"objectiveMetricName\":\"loss\"\n",
    "   },\n",
    "   \"algorithm\":{\n",
    "      \"algorithmName\":\"random\"\n",
    "   },\n",
    "   \"parallelTrialCount\":5,\n",
    "   \"maxFailedTrialCount\":3,\n",
    "   \"parameters\":[\n",
    "      {\n",
    "         \"name\":\"--momentum\",\n",
    "         \"parameterType\":\"double\",\n",
    "         \"feasibleSpace\":{\n",
    "            \"min\":\"0.88\",\n",
    "            \"max\":\"0.92\"\n",
    "         }\n",
    "      },\n",
    "      {\n",
    "         \"name\":\"--decay\",\n",
    "         \"parameterType\":\"double\",\n",
    "         \"feasibleSpace\":{\n",
    "            \"min\":\"0.00049\",\n",
    "            \"max\":\"0.00052\"\n",
    "         }\n",
    "      }\n",
    "   ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert configuration to JSON serialized string format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_tuning_spec = json.dumps(tuning_spec,separators=(',',':'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define pipeline function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus=2 # Number of GPUs to run training\n",
    "\n",
    "def object_detection_pipeline(\n",
    "    s3_path=\"s3://object-det-test/001\",        # AWS S3 bucket URL. Ex: s3://<bucket-name>/ \n",
    "    user_namespace='anonymous',                # User Namespace \n",
    "    timestamp=\"\",                              # Current timestamp\n",
    "    cfg_data=\"voc.data\",                       # Config file containing file name specifications of train, test and validate datasets\n",
    "    cfg_file=\"yolov3-voc.cfg\",                 # Config file containing hyperparameters declarations Ex: yolov3.cfg / yolov4.cfg\n",
    "    weights=\"yolov3-voc_50000.weights\",        # Weights which are already pre-trained upto 50000 iterations is used. Therefore,  \n",
    "                                               # training happens from 50000 iterations upto a limit of max_batches (say 50200) specified \n",
    "                                               # in cfg_file. \n",
    "    trials=2,                                  # Total number of trials under Katib experiment\n",
    "    gpus_per_trial=1,                          # Maximum GPUS to be used for each trial\n",
    "    classes_file=\"voc.names\",                  # File containing the names of object classes (such as person, bus, car,etc)\n",
    "    trained_weights=\"yolov3-voc_final.weights\",# Trained output weights to proceed with validation\n",
    "    max_batches=50050                          # Max batches/No of iterations for training specified in config file\n",
    "):\n",
    "    # Download component\n",
    "    dwn_task = download_op(s3_path=s3_path,\n",
    "                           timestamp=timestamp,\n",
    "                           cfg_data=cfg_data,\n",
    "                           user_namespace=user_namespace\n",
    "                          ).apply(use_aws_secret(secret_name='aws-secret', aws_access_key_id_name='AWS_ACCESS_KEY_ID', aws_secret_access_key_name='AWS_SECRET_ACCESS_KEY'))\n",
    "    dwn_task.add_volume(nfs_volume)\n",
    "    dwn_task.add_volume_mount(nfs_volume_mount) \n",
    "    \n",
    "    # HP tuning (Katib) component\n",
    "    hptuning_task = hptuning_op(s3_path=s3_path,\n",
    "                                cfg_data=cfg_data,             # Config file containing file name specifications of train, test and validate datasets\n",
    "                                cfg_file=cfg_file,             # Config file containing hyperparameters declarations Ex: yolov3.cfg / yolov4.cfg\n",
    "                                weights=weights,               # Pre-trained weights for VOC dataset\n",
    "                                trials=trials,                 # total number of trials under Katib experiment\n",
    "                                timestamp=timestamp,           # Current timestamp to create unique experiment \n",
    "                                                               # Ex: object-detection-1599547688-random-588d7877f5-zvlx5\n",
    "                                gpus_per_trial=gpus_per_trial, # Maximum GPUS to be used for each trial \n",
    "                                max_batches=max_batches,       # Provide Max Batches\n",
    "                                experiment_spec=str_tuning_spec # JSON serialized tuning spec\n",
    "                                ).apply(use_aws_secret(secret_name='aws-secret', aws_access_key_id_name='AWS_ACCESS_KEY_ID', aws_secret_access_key_name='AWS_SECRET_ACCESS_KEY'))\n",
    "    hptuning_task.add_volume(nfs_volume)\n",
    "    hptuning_task.add_volume_mount(nfs_volume_mount)\n",
    "    hptuning_task.after(dwn_task)\n",
    "    \n",
    "    # Train component\n",
    "    train_task = train_op(s3_path=s3_path,\n",
    "                          cfg_data=cfg_data,          # Config file containing file name specifications of train, test and validate datasets\n",
    "                          cfg_file=cfg_file,          # Config file containing hyperparameters declarations Ex: yolov3.cfg / yolov4.cfg\n",
    "                          weights=weights,            # Pre-trained weights for VOC dataset\n",
    "                          gpus=gpus,             \n",
    "                          timestamp=timestamp,\n",
    "                          user_namespace=user_namespace\n",
    "                         ).apply(use_aws_secret(secret_name='aws-secret', aws_access_key_id_name='AWS_ACCESS_KEY_ID', aws_secret_access_key_name='AWS_SECRET_ACCESS_KEY'))\n",
    "    train_task.add_volume(nfs_volume)\n",
    "    train_task.add_volume_mount(nfs_volume_mount).set_gpu_limit(gpus)  #Maximum GPUs to be used for training\n",
    "    train_task.after(hptuning_task)\n",
    "    \n",
    "    # Validation component\n",
    "    validate_task = validate_op(s3_path=s3_path,\n",
    "                                cfg_data=cfg_data,          # Config file containing file name specifications of train, test and validate datasets\n",
    "                                cfg_file=cfg_file,          # Config file containing hyperparameters declarations Ex: yolov3.cfg / yolov4.cfg\n",
    "                                timestamp=timestamp,\n",
    "                                trained_weights=trained_weights\n",
    "                                ).apply(use_aws_secret(secret_name='aws-secret', aws_access_key_id_name='AWS_ACCESS_KEY_ID', aws_secret_access_key_name='AWS_SECRET_ACCESS_KEY'))\n",
    "    validate_task.add_volume(nfs_volume)\n",
    "    validate_task.add_volume_mount(nfs_volume_mount)\n",
    "    validate_task.after(train_task)\n",
    "    \n",
    "    \n",
    "    # Darknet to ncnn conversion component\n",
    "    conversion_ncnn_task = convert_ncnn_op(push_to_s3=\"true\",  # Flag to decide whether to upload the trained weights and converted\n",
    "                                           s3_path=s3_path,    # ncnn model pushed to S3 bucket for future inferencing on anyother \n",
    "                                                               # environment or proceeding to cleanup on UCS (Input: true/false)\n",
    "                                           cfg_data=cfg_data,         \n",
    "                                           cfg_file=cfg_file,\n",
    "                                           weight_file=trained_weights,\n",
    "                                           timestamp=timestamp\n",
    "                                             ).apply(use_aws_secret(secret_name='aws-secret', aws_access_key_id_name='AWS_ACCESS_KEY_ID', aws_secret_access_key_name='AWS_SECRET_ACCESS_KEY'))\n",
    "    conversion_ncnn_task.add_volume(nfs_volume)\n",
    "    conversion_ncnn_task.add_volume_mount(nfs_volume_mount)\n",
    "    conversion_ncnn_task.after(validate_task)\n",
    "    \n",
    "    # Clean up component\n",
    "    cleanup_task = cleanup_op(timestamp=timestamp,\n",
    "                             user_namespace=user_namespace).apply(use_aws_secret(secret_name='aws-secret', aws_access_key_id_name='AWS_ACCESS_KEY_ID', aws_secret_access_key_name='AWS_SECRET_ACCESS_KEY'))\n",
    "    cleanup_task.add_volume(nfs_volume)\n",
    "    cleanup_task.add_volume_mount(nfs_volume_mount)\n",
    "    cleanup_task.after(conversion_ncnn_task)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile pipeline function\n",
    "\n",
    "Compile the pipeline function to create a tar ball for the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.6/site-packages/kfp/components/_data_passing.py:168: UserWarning: Missing type name was inferred as \"Integer\" based on the value \"2\".\n",
      "  warnings.warn('Missing type name was inferred as \"{}\" based on the value \"{}\".'.format(type_name, str(value)))\n",
      "/home/jovyan/.local/lib/python3.6/site-packages/kfp/components/_data_passing.py:168: UserWarning: Missing type name was inferred as \"Integer\" based on the value \"1\".\n",
      "  warnings.warn('Missing type name was inferred as \"{}\" based on the value \"{}\".'.format(type_name, str(value)))\n",
      "/home/jovyan/.local/lib/python3.6/site-packages/kfp/components/_data_passing.py:168: UserWarning: Missing type name was inferred as \"Integer\" based on the value \"50050\".\n",
      "  warnings.warn('Missing type name was inferred as \"{}\" based on the value \"{}\".'.format(type_name, str(value)))\n"
     ]
    }
   ],
   "source": [
    "# Compile pipeline\n",
    "try:\n",
    "    compiler.Compiler().compile(object_detection_pipeline, 'object-detection.tar.gz')\n",
    "except RuntimeError as err:\n",
    "    logging.debug(err)\n",
    "    logging.info(\"Argo workflow failed validation check but it can still be used to run experiments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pipeline experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"/pipeline/#/experiments/details/77521dfc-ff54-4f49-b526-7d581ea527e2\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kp_client = kfp.Client()\n",
    "EXPERIMENT_NAME = 'Object Detection'\n",
    "experiment = kp_client.create_experiment(name=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1613456866'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamp = str(calendar.timegm(time.gmtime()))\n",
    "timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize pipeline parameters & run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Run link <a href=\"/pipeline/#/runs/details/dab062ef-4d62-4ba8-898a-b42d9c0eb3a0\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Pipeline parameters\n",
    "s3_path=\"s3://object-det-test/002\"\n",
    "user_namespace='anonymous'\n",
    "cfg_data=\"voc.data\"              \n",
    "cfg_file=\"yolov3-voc.cfg\"        \n",
    "weights=\"yolov3-voc_50000.weights\"\n",
    "classes_file=\"voc.names\"    \n",
    "trained_weights=\"yolov3-voc_final.weights\" \n",
    "trials=1\n",
    "gpus_per_trial=2\n",
    "\n",
    "\n",
    "run_name = 'object-detection-'+timestamp\n",
    "\n",
    "# Execute pipeline\n",
    "run = kp_client.run_pipeline(experiment.id, run_name,'object-detection.tar.gz', \n",
    "                          params={\"s3_path\": s3_path,\n",
    "                                  \"user_namespace\": user_namespace,\n",
    "                                  \"cfg_data\": cfg_data,\n",
    "                                  \"cfg_file\": cfg_file,\n",
    "                                  \"weights\": weights,\n",
    "                                  \"classes_file\": classes_file,\n",
    "                                  \"trained_weights\": trained_weights,\n",
    "                                  \"timestamp\": timestamp,\n",
    "                                  \"trials\": trials,\n",
    "                                  \"gpus_per_trial\": gpus_per_trial})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve current pipeline run ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dab062ef-4d62-4ba8-898a-b42d9c0eb3a0'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_id = str(run.id)\n",
    "run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline parameters/metrics logging using MLFlow\n",
    "\n",
    "MLFlow's tracking component is used to log input parameters as well as target metrics from the executed pipeline run. \n",
    "\n",
    "Please proceed with logging only after pipeline run is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set MLFlow tracking server URI\n",
    "\n",
    "Tracking server URI is set to log runs at remote location. \n",
    "\n",
    "For external cluster try with http://INGRESS_IP:INGRESS__NODEPORT/mlflow-dashbord/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current tracking uri: http://mlflow-service.kubeflow.svc.cluster.local:80\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"http://mlflow-service.kubeflow.svc.cluster.local:80\")\n",
    "tracking_uri = mlflow.get_tracking_uri()\n",
    "print(\"Current tracking uri: {}\".format(tracking_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve metrics from pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = kp_client.get_run(run.id).run.metrics\n",
    "for metric in metrics:\n",
    "    metric_element = metric.to_dict()\n",
    "    if metric_element['name'] == \"f1-score\":\n",
    "        f1_score = metric_element['number_value']\n",
    "    elif metric_element['name'] == 'map-score':\n",
    "        map_score = metric_element['number_value']\n",
    "    elif metric_element['name'] == 'precision-score':\n",
    "        precision_score = metric_element['number_value']\n",
    "    else:\n",
    "        recall_score = metric_element['number_value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log input parameters and metrics of pipeline run to MLFlow\n",
    "\n",
    "Logged parameters & metrics can be viewed on MLFlow UI at http://INGRESS_IP:INGRESS_NODEPORT/mlflow-dashboard/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create MLFlow experiment using pipeline's run ID\n",
    "experiment_id = mlflow.create_experiment(run_name)\n",
    "experiment = mlflow.get_experiment(experiment_id)\n",
    "\n",
    "#Log params & metrics\n",
    "with mlflow.start_run(experiment_id= experiment.experiment_id, run_name= experiment.name):\n",
    "    mlflow.log_param('Number of training GPUs', gpus)\n",
    "    mlflow.log_param('Timestamp', timestamp)\n",
    "    mlflow.log_param('S3 path', s3_path)\n",
    "    mlflow.log_param('User namespace', user_namespace)\n",
    "    mlflow.log_param('Data config file', cfg_data)\n",
    "    mlflow.log_param('Config file', cfg_file)\n",
    "    mlflow.log_param('Weights', weights)\n",
    "    mlflow.log_param('Trained weights', trained_weights)\n",
    "    mlflow.log_param('Number of Katib trials', trials)\n",
    "    mlflow.log_param('Number of GPUs per trial', gpus_per_trial)\n",
    "    #mlflow.log_params(params)\n",
    "\n",
    "    mlflow.log_metric('Mean Average Precision',map_score)\n",
    "    mlflow.log_metric('Precision score',precision_score)\n",
    "    mlflow.log_metric('Recall score',recall_score)\n",
    "    mlflow.log_metric('F1 score',f1_score)\n",
    "    #mlflow.log_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete MLFlow experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: object-detection-1613456866\n",
      "Artifact Location: file:///tmp/2\n",
      "Lifecycle_stage: deleted\n"
     ]
    }
   ],
   "source": [
    "mlflow.delete_experiment(experiment_id)\n",
    "experiment = mlflow.get_experiment(experiment_id)\n",
    "print(\"Name: {}\".format(experiment.name))\n",
    "print(\"Artifact Location: {}\".format(experiment.artifact_location))\n",
    "print(\"Lifecycle_stage: {}\".format(experiment.lifecycle_stage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline run with run ID 'dab062ef-4d62-4ba8-898a-b42d9c0eb3a0' successfully deleted\n"
     ]
    }
   ],
   "source": [
    "kp_client.runs.delete_run(run_id)\n",
    "print(\"Pipeline run with run ID '%s' successfully deleted\"%run_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
