{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Traffic Dataset for malicious attack\n",
    "\n",
    "This dataset of network traffic flow is generated by CICFlowMeter, indicate whether the traffic is malicious attack (Bot) or not (Benign).                             \n",
    "CICFlowMeter - network traffic flow generator generates 69 statistical features such as Duration, Number of packets, Number of bytes, Length of packets, etc are also calculated separately in the forward and reverse direction.   \n",
    "The output of the application is the CSV file format with two columns labeled for each flow, namely Benign or Bot.\n",
    "The dataset has been organized per day, for each day the raw data including the network traffic (Pcaps) and event logs (windows and Ubuntu event Logs) per machine\n",
    "are recorded.                  Download the dataset from the below wget command line provided and rename as Network_traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -O Network_Traffic.csv https://cse-cic-ids2018.s3.ca-central-1.amazonaws.com/Processed+Traffic+Data+for+ML+Algorithms/Friday-02-03-2018_TrafficForML_CICFlowMeter.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandas --user\n",
    "! pip install imblearn --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart Notebook Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder,MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstZerodrp = ['Timestamp', 'BwdPSHFlags', 'FwdURGFlags', 'BwdURGFlags', 'CWEFlagCount', 'FwdBytsbAvg', 'FwdPktsbAvg',\n",
    "              'FwdBlkRateAvg', 'BwdBytsbAvg',\n",
    "              'BwdBlkRateAvg', 'BwdPktsbAvg']\n",
    "\n",
    "lstScaledrp = ['FwdPSHFlags', 'FINFlagCnt', 'SYNFlagCnt', 'RSTFlagCnt', 'PSHFlagCnt', 'ACKFlagCnt', 'URGFlagCnt',\n",
    "               'ECEFlagCnt']\n",
    "\n",
    "DATA_FILE = \"Network_Traffic.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataFile():\n",
    "    \"\"\"\n",
    "    Reads data file and returns dataframe result\n",
    "    \"\"\"\n",
    "    chunksize = 100000\n",
    "    chunk_list = []\n",
    "    missing_values = [\"n/a\", \"na\", \"--\", \"Infinity\", \"infinity\", \"Nan\", \"NaN\"]\n",
    "\n",
    "    for chunk in pd.read_csv(DATA_FILE, chunksize=chunksize, na_values=missing_values):\n",
    "        chunk_list.append(chunk)\n",
    "#         break\n",
    "    dataFrme = pd.concat(chunk_list)\n",
    "\n",
    "    lstcols = []\n",
    "    for i in dataFrme.columns:\n",
    "        i = str(i).replace(' ', '').replace('/', '')\n",
    "        lstcols.append(i)\n",
    "    dataFrme.columns = lstcols\n",
    "    dfAllCpy = dataFrme.copy()\n",
    "    dataFrme = dataFrme.drop(lstZerodrp, axis=1)\n",
    "    return dataFrme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Traffic Input Dataset \n",
    "\n",
    "### Attribute Information\n",
    "    Features extracted from the captured traffic using CICFlowMeter-V3 = 69\n",
    "    After removal of noise/unwarranted features, number of feature columns chosen: 10\n",
    "    Features: FlowDuration,BwdPktLenMax,FlowIATStd,FwdPSHFlags,BwdPktLenMean,FlowIATMean,BwdIATMean,\n",
    "              FwdSegSizeMin,InitBwdWinByts,BwdPktLenMin\n",
    "    Flows labelled: Bot or Benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DstPort</th>\n",
       "      <th>Protocol</th>\n",
       "      <th>FlowDuration</th>\n",
       "      <th>TotFwdPkts</th>\n",
       "      <th>TotBwdPkts</th>\n",
       "      <th>TotLenFwdPkts</th>\n",
       "      <th>TotLenBwdPkts</th>\n",
       "      <th>FwdPktLenMax</th>\n",
       "      <th>FwdPktLenMin</th>\n",
       "      <th>FwdPktLenMean</th>\n",
       "      <th>...</th>\n",
       "      <th>FwdSegSizeMin</th>\n",
       "      <th>ActiveMean</th>\n",
       "      <th>ActiveStd</th>\n",
       "      <th>ActiveMax</th>\n",
       "      <th>ActiveMin</th>\n",
       "      <th>IdleMean</th>\n",
       "      <th>IdleStd</th>\n",
       "      <th>IdleMax</th>\n",
       "      <th>IdleMin</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>443</td>\n",
       "      <td>6</td>\n",
       "      <td>141385</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>553</td>\n",
       "      <td>3773.0</td>\n",
       "      <td>202</td>\n",
       "      <td>0</td>\n",
       "      <td>61.444444</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49684</td>\n",
       "      <td>6</td>\n",
       "      <td>281</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>443</td>\n",
       "      <td>6</td>\n",
       "      <td>279824</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>1086</td>\n",
       "      <td>10527.0</td>\n",
       "      <td>385</td>\n",
       "      <td>0</td>\n",
       "      <td>98.727273</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>443</td>\n",
       "      <td>6</td>\n",
       "      <td>132</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>443</td>\n",
       "      <td>6</td>\n",
       "      <td>274016</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>1285</td>\n",
       "      <td>6141.0</td>\n",
       "      <td>517</td>\n",
       "      <td>0</td>\n",
       "      <td>142.777778</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   DstPort  Protocol  FlowDuration  TotFwdPkts  TotBwdPkts  TotLenFwdPkts  \\\n",
       "0      443         6        141385           9           7            553   \n",
       "1    49684         6           281           2           1             38   \n",
       "2      443         6        279824          11          15           1086   \n",
       "3      443         6           132           2           0              0   \n",
       "4      443         6        274016           9          13           1285   \n",
       "\n",
       "   TotLenBwdPkts  FwdPktLenMax  FwdPktLenMin  FwdPktLenMean  ...  \\\n",
       "0         3773.0           202             0      61.444444  ...   \n",
       "1            0.0            38             0      19.000000  ...   \n",
       "2        10527.0           385             0      98.727273  ...   \n",
       "3            0.0             0             0       0.000000  ...   \n",
       "4         6141.0           517             0     142.777778  ...   \n",
       "\n",
       "   FwdSegSizeMin  ActiveMean  ActiveStd  ActiveMax  ActiveMin  IdleMean  \\\n",
       "0             20         0.0        0.0        0.0        0.0       0.0   \n",
       "1             20         0.0        0.0        0.0        0.0       0.0   \n",
       "2             20         0.0        0.0        0.0        0.0       0.0   \n",
       "3             20         0.0        0.0        0.0        0.0       0.0   \n",
       "4             20         0.0        0.0        0.0        0.0       0.0   \n",
       "\n",
       "   IdleStd  IdleMax  IdleMin   Label  \n",
       "0      0.0      0.0      0.0  Benign  \n",
       "1      0.0      0.0      0.0  Benign  \n",
       "2      0.0      0.0      0.0  Benign  \n",
       "3      0.0      0.0      0.0  Benign  \n",
       "4      0.0      0.0      0.0  Benign  \n",
       "\n",
       "[5 rows x 69 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_dataFile().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_na(dataFrme):\n",
    "    \"\"\"\n",
    "    Removing NA values\n",
    "    \"\"\"\n",
    "    na_lst = dataFrme.columns[dataFrme.isna().any()].tolist()\n",
    "    for j in na_lst:\n",
    "        dataFrme[j].fillna(0, inplace=True)\n",
    "    return dataFrme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_label(dataFrme):\n",
    "    \"\"\"\n",
    "    Create independent and Dependent Features\n",
    "    \"\"\"\n",
    "    columns = dataFrme.columns.tolist()\n",
    "    # Filter the columns to remove data we do not want \n",
    "    columns = [c for c in columns if c not in [\"Label\"]]\n",
    "    # Store the variable we are predicting \n",
    "    target = \"Label\"\n",
    "    # Define a random state \n",
    "    state = np.random.RandomState(42)\n",
    "    X = dataFrme[columns]\n",
    "    Y = dataFrme[target]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_substitution(dataFrme):\n",
    "    \"\"\"\n",
    "    Label substitution : 'Benign'as 0, 'Bot'as 1\n",
    "    \"\"\"\n",
    "    dictLabel = {'Benign': 0, 'Bot': 1}\n",
    "    dataFrme['Label'] = dataFrme['Label'].map(dictLabel)\n",
    "\n",
    "    LABELS = ['Benign', 'Bot']\n",
    "    count_classes = pd.value_counts(dataFrme['Label'], sort=True)\n",
    "    \n",
    "    # Get the Benign and the Bot values \n",
    "    Benign = dataFrme[dataFrme['Label'] == 0]\n",
    "    Bot = dataFrme[dataFrme['Label'] == 1]\n",
    "    return dataFrme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_class_imbalance(X,Y):\n",
    "    \"\"\"\n",
    "    Handle Class imbalancement \n",
    "    \"\"\"\n",
    "#    os_us = SMOTETomek(ratio=0.5)\n",
    "#    X_res, y_res = os_us.fit_sample(X, Y)\n",
    "    ros = RandomOverSampler(random_state=50)\n",
    "    X_res, y_res = ros.fit_sample(X, Y)\n",
    "    ibtrain_X = pd.DataFrame(X_res,columns=X.columns)\n",
    "    ibtrain_y = pd.DataFrame(y_res,columns=['Label']) \n",
    "    return ibtrain_X,ibtrain_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_features(ibtrain_X):\n",
    "    \"\"\"\n",
    "    Feature Selection - Correlation Ananlysis \n",
    "    \"\"\"\n",
    "    corr = ibtrain_X.corr()\n",
    "    cor_columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "    for i in range(corr.shape[0]):\n",
    "        for j in range(i + 1, corr.shape[0]):\n",
    "            if corr.iloc[i, j] >= 0.9:\n",
    "                if cor_columns[j]:\n",
    "                    cor_columns[j] = False\n",
    "\n",
    "    dfcorr_features = ibtrain_X[corr.columns[cor_columns]]\n",
    "    return dfcorr_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_ten_features(dfcorr_features,ibtrain_X,ibtrain_y):\n",
    "    feat_X = dfcorr_features\n",
    "    feat_y = ibtrain_y['Label']\n",
    "    \n",
    "    #apply SelectKBest class to extract top 10 best features\n",
    "    bestfeatures = SelectKBest(score_func=f_classif, k=10)\n",
    "    fit = bestfeatures.fit(feat_X,feat_y)\n",
    "    dfscores = pd.DataFrame(fit.scores_)\n",
    "    dfcolumns = pd.DataFrame(feat_X.columns)\n",
    "    #concat two dataframes for better visualization \n",
    "    featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "    featureScores.columns = ['Features','Score']  #naming the dataframe columns\n",
    "    final_feature = featureScores.nlargest(10,'Score')['Features'].tolist()\n",
    "    final_feature.sort()\n",
    "    sort_fn = final_feature\n",
    "    dictLabel1 = {'Benign':0,'Bot':1}\n",
    "    ibtrain_y['Label']= ibtrain_y['Label'].map(dictLabel1)\n",
    "    selected_X = ibtrain_X[sort_fn]\n",
    "    selected_Y = ibtrain_y['Label']\n",
    "    return selected_X,selected_Y,sort_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(selected_X, selected_Y):\n",
    "    \"\"\"\n",
    "    Normalize data \n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    selected_X = pd.DataFrame(scaler.fit_transform(selected_X), columns=selected_X.columns, index=selected_X.index)\n",
    "    trainX, testX, trainY, testY = train_test_split(selected_X, selected_Y, test_size=0.25)\n",
    "    print('-----------------------------------------------------------------')\n",
    "    print(\"## Final features and Data pre-process for prediction\")\n",
    "    print('-----------------------------------------------------------------')\n",
    "    print(testX)\n",
    "    return trainX, testX, trainY, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "## Final features and Data pre-process for prediction\n",
      "-----------------------------------------------------------------\n",
      "         BwdPktLenMax  BwdPktLenMean  BwdPktLenMin  FlowDuration  FlowIATMax  \\\n",
      "1438410      0.000000       0.000000      0.000000      0.000004    0.000004   \n",
      "1518446      0.000000       0.000000      0.000000      0.000004    0.000004   \n",
      "1150618      0.000000       0.000000      0.000000      0.000004    0.000004   \n",
      "173401       0.000000       0.000000      0.000000      0.000004    0.000004   \n",
      "844752       0.082192       0.082213      0.083916      0.000424    0.000263   \n",
      "...               ...            ...           ...           ...         ...   \n",
      "1049367      0.000000       0.000000      0.000000      0.000005    0.000005   \n",
      "1167751      0.076712       0.022095      0.000000      0.000090    0.000081   \n",
      "123788       0.039726       0.039736      0.040559      0.000003    0.000003   \n",
      "206238       0.076712       0.022095      0.000000      0.000101    0.000093   \n",
      "673127       0.803425       0.154737      0.000000      0.022662    0.007944   \n",
      "\n",
      "         FwdPktLenMin  FwdSegSizeMin  InitBwdWinByts  Protocol  RSTFlagCnt  \n",
      "1438410      0.000000       0.454545        0.000000  0.352941         0.0  \n",
      "1518446      0.000000       0.454545        0.000000  0.352941         0.0  \n",
      "1150618      0.000000       0.454545        0.000000  0.352941         0.0  \n",
      "173401       0.000000       0.454545        0.000000  0.352941         0.0  \n",
      "844752       0.026027       0.181818        0.000000  1.000000         0.0  \n",
      "...               ...            ...             ...       ...         ...  \n",
      "1049367      0.000000       0.454545        0.000000  0.352941         0.0  \n",
      "1167751      0.000000       0.454545        0.003357  0.352941         1.0  \n",
      "123788       0.028767       0.181818        0.000000  1.000000         0.0  \n",
      "206238       0.000000       0.454545        0.003357  0.352941         1.0  \n",
      "673127       0.000000       0.454545        0.959061  0.352941         1.0  \n",
      "\n",
      "[381192 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    \n",
    "'''Reads data file and returns dataframe result'''\n",
    "dataFrme = read_dataFile()\n",
    "\n",
    "''' Removing NA values'''\n",
    "dataFrme = preprocess_na(dataFrme)\n",
    "\n",
    "'''Create independent and Dependent Features'''\n",
    "X, Y = create_features_label(dataFrme)\n",
    "\n",
    "'''Label substitution : 'Benign'as 0, 'Bot'as 1'''\n",
    "dataFrme = label_substitution(dataFrme)\n",
    "\n",
    "'''Handle Class imbalancement'''\n",
    "ibtrain_X, ibtrain_y = handle_class_imbalance(X, Y)\n",
    "\n",
    "'''Feature Selection - Correlation Ananlysis'''\n",
    "dfcorr_features = correlation_features(ibtrain_X)\n",
    "\n",
    "'''Feature Selection - SelectKBest : Return best 10 features'''\n",
    "selected_X, selected_Y, final_feature = top_ten_features(dfcorr_features, ibtrain_X, ibtrain_y)\n",
    "\n",
    "'''Normalize data '''\n",
    "trainX, testX, trainY, testY = normalize_data(selected_X, selected_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of Serving Input Receiver Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_cols():\n",
    "  input_columns = [tf.feature_column.numeric_column(k,dtype=tf.dtypes.float64) for k in final_feature]\n",
    "  return input_columns\n",
    "feature_columns =  make_feature_cols()\n",
    "inputs = {}\n",
    "for feat in feature_columns:\n",
    "  inputs[feat.name] = tf.placeholder(shape=[None], dtype=feat.dtype)\n",
    "serving_input_receiver_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Save Network Traffic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_DATA_DIR = os.getenv(\"TF_DATA_DIR\", \"/tmp/data/\")\n",
    "TF_MODEL_DIR = os.getenv(\"TF_MODEL_DIR\", \"network/\")\n",
    "TF_EXPORT_DIR = os.getenv(\"TF_EXPORT_DIR\", \"network/\")\n",
    "\n",
    "x1 = np.asarray(trainX[final_feature])\n",
    "y1 = np.asarray(trainY)\n",
    "\n",
    "x2 = np.asarray(testX[final_feature])\n",
    "y2 = np.asarray(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatFeatures(features):\n",
    "    formattedFeatures = {}\n",
    "    numColumns = features.shape[1]\n",
    "\n",
    "    for i in range(0, numColumns):\n",
    "        formattedFeatures[final_feature[i]] = features[:, i]\n",
    "\n",
    "    return formattedFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:ParameterServerStrategy with compute_devices = ('/device:GPU:0', '/device:GPU:1'), variable_device = '/device:CPU:0'\n",
      "Number of devices: 2\n",
      "INFO:tensorflow:Initializing RunConfig with distribution strategies.\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'network/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.python.distribute.parameter_server_strategy.ParameterServerStrategyV1 object at 0x7f561c486358>, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f561c486e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': None}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1000 or save_checkpoints_secs None.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/canned/head.py:437: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Reduce to /device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "WARNING:tensorflow:Issue encountered when serializing variables.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "\n",
      "WARNING:tensorflow:Issue encountered when serializing trainable_variables.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "\n",
      "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:Issue encountered when serializing variables.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "\n",
      "WARNING:tensorflow:Issue encountered when serializing trainable_variables.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "\n",
      "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "\n",
      "INFO:tensorflow:Saving checkpoints for 0 into network/model.ckpt.\n",
      "WARNING:tensorflow:Issue encountered when serializing variables.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "\n",
      "WARNING:tensorflow:Issue encountered when serializing trainable_variables.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "\n",
      "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "\n",
      "INFO:tensorflow:loss = 45.72928, step = 0\n",
      "INFO:tensorflow:global_step/sec: 156.045\n",
      "INFO:tensorflow:loss = 12.234756, step = 100 (0.644 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.347\n",
      "INFO:tensorflow:loss = 13.981091, step = 200 (0.511 sec)\n",
      "INFO:tensorflow:global_step/sec: 201.965\n",
      "INFO:tensorflow:loss = 11.761612, step = 300 (0.495 sec)\n",
      "INFO:tensorflow:global_step/sec: 195.029\n",
      "INFO:tensorflow:loss = 15.587199, step = 400 (0.512 sec)\n",
      "INFO:tensorflow:global_step/sec: 94.8191\n",
      "INFO:tensorflow:loss = 15.9000025, step = 500 (1.057 sec)\n",
      "INFO:tensorflow:global_step/sec: 204.48\n",
      "INFO:tensorflow:loss = 15.760313, step = 600 (0.486 sec)\n",
      "INFO:tensorflow:global_step/sec: 209.939\n",
      "INFO:tensorflow:loss = 6.8568716, step = 700 (0.476 sec)\n",
      "INFO:tensorflow:global_step/sec: 208.455\n",
      "INFO:tensorflow:loss = 19.205095, step = 800 (0.480 sec)\n",
      "INFO:tensorflow:global_step/sec: 206.425\n",
      "INFO:tensorflow:loss = 14.903017, step = 900 (0.485 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into network/model.ckpt.\n",
      "WARNING:tensorflow:Issue encountered when serializing variables.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "\n",
      "WARNING:tensorflow:Issue encountered when serializing trainable_variables.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "\n",
      "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/metrics_impl.py:2026: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2020-07-09T11:44:18Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from network/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2020-07-09-11:44:20\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.89375, accuracy_baseline = 0.5046875, auc = 0.95857364, auc_precision_recall = 0.96306574, average_loss = 0.22439714, global_step = 1000, label/mean = 0.4953125, loss = 7.1807084, precision = 0.8243877, prediction/mean = 0.5017197, recall = 0.99810725\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: network/model.ckpt-1000\n",
      "INFO:tensorflow:Performing the final export in the end of training.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\n",
      "INFO:tensorflow:'serving_default' : Classification input must be a single string Tensor; got {'BwdPktLenMax': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=float64>, 'BwdPktLenMean': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float64>, 'BwdPktLenMin': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=float64>, 'FlowDuration': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float64>, 'FlowIATMax': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=float64>, 'FwdPktLenMin': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=float64>, 'FwdSegSizeMin': <tf.Tensor 'Placeholder_6:0' shape=(?,) dtype=float64>, 'InitBwdWinByts': <tf.Tensor 'Placeholder_7:0' shape=(?,) dtype=float64>, 'Protocol': <tf.Tensor 'Placeholder_8:0' shape=(?,) dtype=float64>, 'RSTFlagCnt': <tf.Tensor 'Placeholder_9:0' shape=(?,) dtype=float64>}\n",
      "INFO:tensorflow:'classification' : Classification input must be a single string Tensor; got {'BwdPktLenMax': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=float64>, 'BwdPktLenMean': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float64>, 'BwdPktLenMin': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=float64>, 'FlowDuration': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float64>, 'FlowIATMax': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=float64>, 'FwdPktLenMin': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=float64>, 'FwdSegSizeMin': <tf.Tensor 'Placeholder_6:0' shape=(?,) dtype=float64>, 'InitBwdWinByts': <tf.Tensor 'Placeholder_7:0' shape=(?,) dtype=float64>, 'Protocol': <tf.Tensor 'Placeholder_8:0' shape=(?,) dtype=float64>, 'RSTFlagCnt': <tf.Tensor 'Placeholder_9:0' shape=(?,) dtype=float64>}\n",
      "INFO:tensorflow:'regression' : Regression input must be a single string Tensor; got {'BwdPktLenMax': <tf.Tensor 'Placeholder:0' shape=(?,) dtype=float64>, 'BwdPktLenMean': <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float64>, 'BwdPktLenMin': <tf.Tensor 'Placeholder_2:0' shape=(?,) dtype=float64>, 'FlowDuration': <tf.Tensor 'Placeholder_3:0' shape=(?,) dtype=float64>, 'FlowIATMax': <tf.Tensor 'Placeholder_4:0' shape=(?,) dtype=float64>, 'FwdPktLenMin': <tf.Tensor 'Placeholder_5:0' shape=(?,) dtype=float64>, 'FwdSegSizeMin': <tf.Tensor 'Placeholder_6:0' shape=(?,) dtype=float64>, 'InitBwdWinByts': <tf.Tensor 'Placeholder_7:0' shape=(?,) dtype=float64>, 'Protocol': <tf.Tensor 'Placeholder_8:0' shape=(?,) dtype=float64>, 'RSTFlagCnt': <tf.Tensor 'Placeholder_9:0' shape=(?,) dtype=float64>}\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:Restoring parameters from network/model.ckpt-1000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: network/export/network/temp-b'1594295060'/saved_model.pb\n",
      "INFO:tensorflow:Loss for final step: 12.153394.\n",
      "({'accuracy': 0.89375, 'accuracy_baseline': 0.5046875, 'auc': 0.95857364, 'auc_precision_recall': 0.96306574, 'average_loss': 0.22439714, 'label/mean': 0.4953125, 'loss': 7.1807084, 'precision': 0.8243877, 'prediction/mean': 0.5017197, 'recall': 0.99810725, 'global_step': 1000}, [b'network/export/network/1594295060'])\n",
      "Training finished successfully\n"
     ]
    }
   ],
   "source": [
    "trainingFeatures = formatFeatures(x1)\n",
    "trainingCategories = y1\n",
    "\n",
    "testFeatures = formatFeatures(x2)\n",
    "testCategories = y2\n",
    "\n",
    "# Train Input Function\n",
    "def train_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((trainingFeatures, y1))\n",
    "    dataset = dataset.batch(32).repeat(1000)\n",
    "    return dataset\n",
    "\n",
    "# Test Input Function\n",
    "def eval_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((testFeatures, y2))\n",
    "    return dataset.batch(32).repeat(1000)\n",
    "\n",
    "\n",
    "# Provide list of GPUs should be used to train the model\n",
    "\n",
    "distribution=tf.distribute.experimental.ParameterServerStrategy()\n",
    "print('Number of devices: {}'.format(distribution.num_replicas_in_sync))\n",
    "\n",
    "# Configuration of  training model\n",
    "\n",
    "config = tf.estimator.RunConfig(train_distribute=distribution, model_dir=TF_MODEL_DIR, save_summary_steps=100, save_checkpoints_steps=1000)\n",
    "\n",
    "# Build 3 layer DNN classifier\n",
    "\n",
    "model = tf.estimator.DNNClassifier(hidden_units=[13,65,110],\n",
    "                                   feature_columns=feature_columns,\n",
    "                                   model_dir=TF_MODEL_DIR,\n",
    "                                   n_classes=2, config=config\n",
    "                                   )\n",
    "\n",
    "\n",
    "export_final = tf.estimator.FinalExporter(TF_EXPORT_DIR, serving_input_receiver_fn=serving_input_receiver_fn)\n",
    "\n",
    "train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn,\n",
    "                                    max_steps=1000)\n",
    "\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn,\n",
    "                                  steps=100,\n",
    "                                  exporters=export_final,\n",
    "                                  throttle_secs=1,\n",
    "                                  start_delay_secs=1)\n",
    "\n",
    "result = tf.estimator.train_and_evaluate(model, train_spec, eval_spec)\n",
    "print(result)\n",
    "\n",
    "print('Training finished successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update  storageUri in network_kfserving.yaml with pvc-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvcname = !(echo  $HOSTNAME | sed 's/.\\{2\\}$//')\n",
    "pvc = \"workspace-\"+pvcname[0]\n",
    "! sed -i \"s/nfs/$pvc/g\" network_kfserving.yaml\n",
    "! cat network_kfserving.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving Network Traffic Model using kubeflow kfserving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl apply -f network_kfserving.yaml -n anonymous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get inferenceservices -n anonymous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "Wait for inference service READY=\\\"True\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict data from serving after setting INGRESS_IP\n",
    "### Note - Use one of preprocessed row values from Data pre-process from prediction output cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -v -H \"Host: network-model.anonymous.example.com\" http://10.23.222.166:31380/v1/models/network-model:predict -d '{\"signature_name\":\"predict\",\"instances\":[{\"BwdPktLenMax\":[0.158904] , \"BwdPktLenMean\":[0.039736] , \"BwdPktLenMin\":[0.00000], \"FlowDuration\":[0.053778] , \"FlowIATMax\":[0.053262] , \"FwdPktLenMin\":[0.0] , \"FwdSegSizeMin\":[0.454545] , \"InitBwdWinByts\":[1.0] , \"Protocol\":[0.0] , \"RSTFlagCnt\":[0.003357]}]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete kfserving model & Clean up of stored models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete -f network_kfserving.yaml\n",
    "!rm -rf /mnt/network\n",
    "pvcname = !(echo  $HOSTNAME | sed 's/.\\{2\\}$//')\n",
    "pvc = \"workspace-\"+pvcname[0]\n",
    "! sed -i \"s/$pvc/nfs/g\" network_kfserving.yaml\n",
    "! cat network_kfserving.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
