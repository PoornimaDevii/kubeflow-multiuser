{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLE-RSSI Dataset for Indoor localization \n",
    "\n",
    "The dataset was created using the RSSI readings of an array of 13 ibeacons in the first floor of Waldo Library, Western Michigan University. Data was collected using iPhone 6S. The dataset contains two sub-datasets: a labeled dataset (1420 instances) and an unlabeled dataset (5191 instances). The recording was performed during the operational hours of the library. For the labeled dataset, the input data contains the location (label column), a timestamp, followed by RSSI readings of 13 iBeacons. RSSI measurements are negative values. Bigger RSSI values indicate closer proximity to a given iBeacon (e.g., RSSI of -65 represent a closer distance to a given iBeacon compared to RSSI of -85). For out-of-range iBeacons, the RSSI is indicated by -200. The locations related to RSSI readings are combined in one column consisting a letter for the column and a number for the row of the position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](pictures/iBeacon_Layout.jpg \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone Cisco Kubeflow Starter pack repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BRANCH_NAME=\"master\" #Provide git branch name \"master\" or \"dev\"\n",
    "! git clone -b $BRANCH_NAME https://github.com/CiscoAI/cisco-kubeflow-starter-pack.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "pandas\n",
    "numpy\n",
    "seldon-core\n",
    "tornado>=6.0.3\n",
    "kubeflow-tfjob\n",
    "kubeflow-fairing\n",
    "tensorflow==1.14.0\n",
    "kubernetes==10.0.1\n",
    "minio\n",
    "kubeflow-katib\n",
    "sklearn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart Notebook kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set name of model store\n",
    "\n",
    "Set model_store variable to either 'minio' or 'workspace-vol' to store your trained model files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_store = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not model_store or model_store not in ('minio','workspace-vol'):\n",
    "     raise ValueError(\"Set the name of the model store to be used: minio/workspace-vol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "import yaml\n",
    "from minio import Minio\n",
    "from kubernetes import client as k8s_client\n",
    "from kubernetes.client import rest as k8s_rest\n",
    "from kubernetes import config as k8s_config\n",
    "from kubernetes.client.rest import ApiException\n",
    "from kubeflow.fairing.cloud.k8s import MinioUploader\n",
    "from kubeflow.fairing.builders.cluster.minio_context import MinioContextSource\n",
    "from kubernetes.client import V1PodTemplateSpec\n",
    "from kubernetes.client import V1ObjectMeta\n",
    "from kubernetes.client import V1PodSpec\n",
    "from kubernetes.client import V1Container\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import kubeflow.katib as kc\n",
    "from kubeflow.katib import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create minIO secret & serviceaccount\n",
    "\n",
    "Create a Kubernetes Secret with MinIO credentials, and a separate service account if case the model store chosen is 'minio'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_store == 'minio':\n",
    "    \n",
    "    # Create MinIO secret\n",
    "    minio_secret = f\"\"\"apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: miniosecret\n",
    "  annotations:\n",
    "     serving.kubeflow.org/s3-endpoint: minio-service.kubeflow:9000 # replace with your s3 endpoint\n",
    "     serving.kubeflow.org/s3-usehttps: \"0\" # by default 1, for testing with minio you need to set to 0\n",
    "type: Opaque\n",
    "stringData:\n",
    "  awsAccessKeyID: minio\n",
    "  awsSecretAccessKey: minio123\n",
    "\"\"\"\n",
    "\n",
    "    minio_secret = yaml.safe_load(minio_secret)\n",
    "    with open('minio-secret.yaml', 'w') as file:\n",
    "        yaml_doc_secret = yaml.dump(minio_secret,file)\n",
    "        \n",
    "    print(\"Creating Minio secret..... \")\n",
    "    \n",
    "    !kubectl apply -f minio-secret.yaml -n anonymous\n",
    "    \n",
    "    !kubectl get secrets -n anonymous | grep miniosecret\n",
    "    \n",
    "    #Create MinIO service account\n",
    "    minio_serviceaccount = f\"\"\"apiVersion: v1\n",
    "kind: ServiceAccount\n",
    "metadata:\n",
    "  name: minio-sa\n",
    "secrets:\n",
    "- name: miniosecret\n",
    "\"\"\"\n",
    "    \n",
    "    minio_serviceaccount = yaml.safe_load(minio_serviceaccount)\n",
    "    with open('minio-serviceaccount.yaml', 'w') as file:\n",
    "        yaml_doc_sa = yaml.dump(minio_serviceaccount,file)\n",
    "\n",
    "    print(\"Creating Minio service account.....\")\n",
    "    \n",
    "    !kubectl apply -f minio-serviceaccount.yaml -n anonymous\n",
    "    \n",
    "    !kubectl get serviceaccount -n anonymous | grep minio-sa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to minIO service & create MinIO bucket\n",
    "\n",
    "Connects to minIO service and returns a service endpoint. Also creates a MinIO bucket if model store chosen is 'minio'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_store == 'minio':\n",
    "        \n",
    "        #Connect to minIO service using credentials\n",
    "        k8s_config.load_incluster_config()\n",
    "        api_client = k8s_client.CoreV1Api()\n",
    "        minio_service_endpoint = None\n",
    "\n",
    "        try:\n",
    "            minio_service_endpoint = api_client.read_namespaced_service(name='minio-service', namespace='kubeflow').spec.cluster_ip\n",
    "        except ApiException as e:\n",
    "            if e.status == 403:\n",
    "                logging.warning(f\"The service account doesn't have sufficient privileges \"\n",
    "                              f\"to get the kubeflow minio-service. \"\n",
    "                              f\"You will have to manually enter the minio cluster-ip. \"\n",
    "                              f\"To make this function work ask someone with cluster \"\n",
    "                              f\"priveleges to create an appropriate \"\n",
    "                              f\"clusterrolebinding by running a command.\\n\"\n",
    "                              f\"kubectl create --namespace=kubeflow rolebinding \"\n",
    "                               \"--clusterrole=kubeflow-view \"\n",
    "                               \"--serviceaccount=${NAMESPACE}:default-editor \"\n",
    "                               \"${NAMESPACE}-minio-view\")\n",
    "                logging.error(\"API access denied with reason: {e.reason}\")\n",
    "\n",
    "        s3_endpoint = minio_service_endpoint\n",
    "        s3_endPoint = s3_endpoint+\":9000\"\n",
    "        minio_endpoint = \"http://\"+s3_endPoint\n",
    "        minio_username = \"minio\"\n",
    "        minio_key = \"minio123\"\n",
    "        minio_region = \"us-east-1\"\n",
    "        print(\"Minio Endpoint returned:\", minio_endpoint)\n",
    "        \n",
    "        # Define MinIO uploader\n",
    "        minio_uploader = MinioUploader(endpoint_url=minio_endpoint, minio_secret=minio_username, minio_secret_key=minio_key, region_name=minio_region)\n",
    "        \n",
    "        # Create MinIO bucket\n",
    "        minio_bucket = \"minioblerssi\"\n",
    "        minio_uploader.create_bucket(minio_bucket)\n",
    "        print(\"Minio bucket of %s created successfully or already exists\" %minio_bucket)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate hyper-parameter  values using Katib SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithmsettings = V1alpha3AlgorithmSetting(\n",
    "    name= \"random_state\",\n",
    "    value = \"10\"\n",
    "    )\n",
    "algorithm = V1alpha3AlgorithmSpec(\n",
    "    algorithm_name = \"random\",\n",
    "    algorithm_settings = [algorithmsettings]\n",
    "  )\n",
    "\n",
    "# Metric Collector\n",
    "collector = V1alpha3CollectorSpec(kind = \"TensorFlowEvent\")\n",
    "FileSystemPath = V1alpha3FileSystemPath(kind = \"/train\" , path = \"Directory\")\n",
    "metrics_collector_spec = V1alpha3MetricsCollectorSpec(\n",
    "    collector = collector,\n",
    "    source = FileSystemPath)\n",
    "\n",
    "# Objective\n",
    "objective = V1alpha3ObjectiveSpec(\n",
    "    goal = 0.5,\n",
    "    objective_metric_name = \"accuracy\",\n",
    "    additional_metric_names= [\"Train-accuracy\"],\n",
    "    type = \"maximize\")\n",
    "\n",
    "# Parameters\n",
    "\n",
    "feasible_space_batchsize = V1alpha3FeasibleSpace(list = [\"16\",\"32\",\"48\",\"64\"])\n",
    "feasible_space_lr = V1alpha3FeasibleSpace(min = \"0.01\", max = \"0.03\")\n",
    "\n",
    "parameters = [V1alpha3ParameterSpec(\n",
    "    feasible_space = feasible_space_batchsize,\n",
    "    name = \"--batch-size\",\n",
    "    parameter_type = \"categorical\"\n",
    "    ),\n",
    "    V1alpha3ParameterSpec(\n",
    "    feasible_space = feasible_space_lr, \n",
    "    name = \"--learning-rate\",\n",
    "    parameter_type =\"double\"\n",
    "    )]\n",
    "\n",
    "# Trialtemplate\n",
    "go_template = V1alpha3GoTemplate(\n",
    "    raw_template =   \"apiVersion: \\\"batch/v1\\\"\\nkind: Job\\nmetadata:\\n  name: {{.Trial}}\\n  namespace: {{.NameSpace}}\\nspec:\\n  template:\\n    spec:\\n      containers:\\n      - name: {{.Trial}}\\n        image: docker.io/poornimadevii/blerssi-train:v1\\n        command:\\n        - \\\"python3\\\"\\n        - \\\"/opt/blerssi-model.py\\\"\\n        {{- with .HyperParameters}}\\n        {{- range .}}\\n        - \\\"{{.Name}}={{.Value}}\\\"\\n        {{- end}}\\n        {{- end}}\\n      restartPolicy: Never\"\n",
    "    )\n",
    "\n",
    "\n",
    "trial_template= V1alpha3TrialTemplate(go_template=go_template)\n",
    "\n",
    "\n",
    "# Experiment\n",
    "experiment = V1alpha3Experiment(\n",
    "    api_version=\"kubeflow.org/v1alpha3\",\n",
    "    kind=\"Experiment\",\n",
    "    metadata=V1ObjectMeta(name=\"blerssi-dnn\",namespace=\"anonymous\"),\n",
    "\n",
    "    spec=V1alpha3ExperimentSpec(\n",
    "         algorithm = algorithm,\n",
    "         max_failed_trial_count=3,\n",
    "         max_trial_count=5,\n",
    "         objective = objective,\n",
    "         parallel_trial_count=5,\n",
    "         parameters = parameters ,\n",
    "         trial_template = trial_template\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create katib experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namespace = kc.utils.get_default_target_namespace()\n",
    "kclient = kc.KatibClient()\n",
    "kclient.create_experiment(experiment, namespace=namespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check katib experiment succeeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kclient.is_experiment_succeeded(name=\"blerssi-dnn\", namespace=namespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note : \n",
    "Check status of katib experiment and wait till katib experiment gets completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get optimal hyper-parameter of an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kclient.get_optimal_hyperparmeters(name=\"blerssi-dnn\",namespace=namespace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare and assign hyperparameter variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = kclient.get_optimal_hyperparameters(name=\"blerssi-dnn\",namespace=namespace)\n",
    "batchsize = parameter['currentOptimalTrial']['parameterAssignments'][0]['value']\n",
    "learningrate = parameter['currentOptimalTrial']['parameterAssignments'][1]['value']\n",
    "TF_BATCH_SIZE = int(batchsize)\n",
    "TF_LEARNING_RATE = float(learningrate)\n",
    "print(TF_BATCH_SIZE)\n",
    "print(TF_LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPUs availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = len(tf.config.experimental.list_physical_devices('GPU'))\n",
    "if gpus == 0:\n",
    "    print(\"Model will be trained using CPU\")\n",
    "elif gpus >= 0:\n",
    "    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "    tf.config.experimental.list_physical_devices('GPU')\n",
    "    print(\"Model will be trained using GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare variables for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'cisco-kubeflow-starter-pack/apps/networking/ble-localization/onprem/data/'\n",
    "BLE_RSSI = pd.read_csv(os.path.join(path, 'iBeacon_RSSI_Labeled.csv')) #Labeled dataset\n",
    "                       \n",
    "# Configure model options\n",
    "TF_DATA_DIR = os.getenv(\"TF_DATA_DIR\", \"/tmp/data/\")\n",
    "TF_MODEL_DIR = os.getenv(\"TF_MODEL_DIR\", \"blerssi/\")\n",
    "TF_EXPORT_DIR = os.getenv(\"TF_EXPORT_DIR\", \"blerssi/\")\n",
    "TF_MODEL_TYPE = os.getenv(\"TF_MODEL_TYPE\", \"DNN\")\n",
    "TF_TRAIN_STEPS = int(os.getenv(\"TF_TRAIN_STEPS\", 5000))\n",
    "\n",
    "\n",
    "# Feature columns\n",
    "COLUMNS = list(BLE_RSSI.columns)\n",
    "FEATURES = COLUMNS[2:]\n",
    "def make_feature_cols():\n",
    "  input_columns = [tf.feature_column.numeric_column(k) for k in FEATURES]\n",
    "  return input_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLERSSI Input Dataset\n",
    "\n",
    "### Attribute Information\n",
    "\n",
    "    location: The location of receiving RSSIs from ibeacons b3001 to b3013; \n",
    "              symbolic values showing the column and row of the location on the map (e.g., A01 stands for column A, row 1).\n",
    "    date: Datetime in the format of ‘d-m-yyyy hh:mm:ss’\n",
    "    b3001 - b3013: RSSI readings corresponding to the iBeacons; numeric, integers only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLE_RSSI.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define serving input receiver function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns =  make_feature_cols()\n",
    "inputs = {}\n",
    "for feat in feature_columns:\n",
    "  inputs[feat.name] = tf.placeholder(shape=[None], dtype=feat.dtype)\n",
    "serving_input_receiver_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and save BLE-RSSI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature columns\n",
    "COLUMNS = list(BLE_RSSI.columns)\n",
    "FEATURES = COLUMNS[2:]\n",
    "LABEL = [COLUMNS[0]]\n",
    "\n",
    "b3001 = tf.feature_column.numeric_column(key='b3001',dtype=tf.float64)\n",
    "b3002 = tf.feature_column.numeric_column(key='b3002',dtype=tf.float64)\n",
    "b3003 = tf.feature_column.numeric_column(key='b3003',dtype=tf.float64)\n",
    "b3004 = tf.feature_column.numeric_column(key='b3004',dtype=tf.float64)\n",
    "b3005 = tf.feature_column.numeric_column(key='b3005',dtype=tf.float64)\n",
    "b3006 = tf.feature_column.numeric_column(key='b3006',dtype=tf.float64)\n",
    "b3007 = tf.feature_column.numeric_column(key='b3007',dtype=tf.float64)\n",
    "b3008 = tf.feature_column.numeric_column(key='b3008',dtype=tf.float64)\n",
    "b3009 = tf.feature_column.numeric_column(key='b3009',dtype=tf.float64)\n",
    "b3010 = tf.feature_column.numeric_column(key='b3010',dtype=tf.float64)\n",
    "b3011 = tf.feature_column.numeric_column(key='b3011',dtype=tf.float64)\n",
    "b3012 = tf.feature_column.numeric_column(key='b3012',dtype=tf.float64)\n",
    "b3013 = tf.feature_column.numeric_column(key='b3013',dtype=tf.float64)\n",
    "feature_columns = [b3001, b3002, b3003, b3004, b3005, b3006, b3007, b3008, b3009, b3010, b3011, b3012, b3013]\n",
    "\n",
    "df_full = pd.read_csv(os.path.join(path, \"iBeacon_RSSI_Labeled.csv\")) #Labeled dataset\n",
    "\n",
    "# Input Data Preprocessing \n",
    "df_full = df_full.drop(['date'],axis = 1)\n",
    "df_full[FEATURES] = (df_full[FEATURES])/(-200)\n",
    "\n",
    "\n",
    "#Output Data Preprocessing\n",
    "dict = {'O02': 0,'P01': 1,'P02': 2,'R01': 3,'R02': 4,'S01': 5,'S02': 6,'T01': 7,'U02': 8,'U01': 9,'J03': 10,'K03': 11,'L03': 12,'M03': 13,'N03': 14,'O03': 15,'P03': 16,'Q03': 17,'R03': 18,'S03': 19,'T03': 20,'U03': 21,'U04': 22,'T04': 23,'S04': 24,'R04': 25,'Q04': 26,'P04': 27,'O04': 28,'N04': 29,'M04': 30,'L04': 31,'K04': 32,'J04': 33,'I04': 34,'I05': 35,'J05': 36,'K05': 37,'L05': 38,'M05': 39,'N05': 40,'O05': 41,'P05': 42,'Q05': 43,'R05': 44,'S05': 45,'T05': 46,'U05': 47,'S06': 48,'R06': 49,'Q06': 50,'P06': 51,'O06': 52,'N06': 53,'M06': 54,'L06': 55,'K06': 56,'J06': 57,'I06': 58,'F08': 59,'J02': 60,'J07': 61,'I07': 62,'I10': 63,'J10': 64,'D15': 65,'E15': 66,'G15': 67,'J15': 68,'L15': 69,'R15': 70,'T15': 71,'W15': 72,'I08': 73,'I03': 74,'J08': 75,'I01': 76,'I02': 77,'J01': 78,'K01': 79,'K02': 80,'L01': 81,'L02': 82,'M01': 83,'M02': 84,'N01': 85,'N02': 86,'O01': 87,'I09': 88,'D14': 89,'D13': 90,'K07': 91,'K08': 92,'N15': 93,'P15': 94,'I15': 95,'S15': 96,'U15': 97,'V15': 98,'S07': 99,'S08': 100,'L09': 101,'L08': 102,'Q02': 103,'Q01': 104}\n",
    "df_full['location'] = df_full['location'].map(dict)\n",
    "df_train=df_full.sample(frac=0.8,random_state=200)\n",
    "df_valid=df_full.drop(df_train.index)\n",
    "\n",
    "location_counts = BLE_RSSI.location.value_counts()\n",
    "x1 = np.asarray(df_train[FEATURES])\n",
    "y1 = np.asarray(df_train['location'])\n",
    "\n",
    "x2 = np.asarray(df_valid[FEATURES])\n",
    "y2 = np.asarray(df_valid['location'])\n",
    "\n",
    "def formatFeatures(features):\n",
    "    formattedFeatures = {}\n",
    "    numColumns = features.shape[1]\n",
    "\n",
    "    for i in range(0, numColumns):\n",
    "        formattedFeatures[\"b\"+str(3001+i)] = features[:, i]\n",
    "\n",
    "    return formattedFeatures\n",
    "\n",
    "trainingFeatures = formatFeatures(x1)\n",
    "trainingCategories = y1\n",
    "\n",
    "testFeatures = formatFeatures(x2)\n",
    "testCategories = y2\n",
    "\n",
    "# Train Input Function\n",
    "def train_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((trainingFeatures, y1))\n",
    "    dataset = dataset.repeat(1000).batch(TF_BATCH_SIZE)\n",
    "    return dataset\n",
    "\n",
    "# Test Input Function\n",
    "def eval_input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((testFeatures, y2))\n",
    "    return dataset.repeat(1000).batch(TF_BATCH_SIZE)\n",
    "\n",
    "# Provide list of GPUs should be used to train the model\n",
    "\n",
    "distribution=tf.distribute.experimental.ParameterServerStrategy()\n",
    "print('Number of devices: {}'.format(distribution.num_replicas_in_sync))\n",
    "\n",
    "# Configuration of  training model\n",
    "\n",
    "config = tf.estimator.RunConfig(train_distribute=distribution, model_dir=TF_MODEL_DIR, save_summary_steps=100, save_checkpoints_steps=100)\n",
    "\n",
    "# Build 3 layer DNN classifier\n",
    "\n",
    "model = tf.estimator.DNNClassifier(hidden_units = [13,65,110],\n",
    "                 feature_columns = feature_columns,\n",
    "                 model_dir = TF_MODEL_DIR,\n",
    "                 n_classes=105, config=config\n",
    "               )\n",
    "\n",
    "export_final = tf.estimator.FinalExporter(TF_EXPORT_DIR, serving_input_receiver_fn=serving_input_receiver_fn)\n",
    "\n",
    "train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, \n",
    "                                    max_steps=TF_TRAIN_STEPS)\n",
    "\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn,\n",
    "                                  steps=100,\n",
    "                                  exporters=export_final,\n",
    "                                  throttle_secs=1,\n",
    "                                  start_delay_secs=1)\n",
    "\n",
    "# Train and Evaluate the model\n",
    "\n",
    "tf.estimator.train_and_evaluate(model, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload model folder & content into MinIO bucket\n",
    "\n",
    "Upload the model folder that is saved in the declared location above, into the created MinIO bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_store == 'minio':\n",
    "    \n",
    "    # Create a MinIO client\n",
    "    minioClient = Minio(s3_endPoint,\n",
    "                    access_key='minio',\n",
    "                    secret_key='minio123',\n",
    "                    secure=False)\n",
    "    \n",
    "    # Upload the model folder & contents into MinIO bucket\n",
    "    initial_dir='export/' + TF_EXPORT_DIR\n",
    "    for dir in os.listdir(os.path.join(TF_MODEL_DIR, initial_dir)):\n",
    "        if re.match( \"^[0-9]+$\", dir):\n",
    "            for subdir in os.listdir(os.path.join(TF_MODEL_DIR, initial_dir, dir)):\n",
    "                if subdir=='variables':\n",
    "                    for file in os.listdir(os.path.join(TF_MODEL_DIR, initial_dir, dir, subdir)):\n",
    "                        obj_name = TF_EXPORT_DIR + dir + '/' + subdir + '/' + file\n",
    "                        print(minioClient.fput_object(minio_bucket, obj_name ,os.path.join(TF_MODEL_DIR, initial_dir, dir, subdir, file)))\n",
    "                else:\n",
    "                    obj_name = TF_EXPORT_DIR + dir + '/' + subdir\n",
    "                    print(minioClient.fput_object(minio_bucket, obj_name, os.path.join(TF_MODEL_DIR, initial_dir, dir, subdir)))\n",
    "                    \n",
    "    # List objects stored in minIO bucket\n",
    "    model_response = minio_uploader.client.list_objects(Bucket=minio_bucket)\n",
    "    print(\"List of objects as stored in MinIO bucket:\\n\\t\")\n",
    "    print(model_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define inference service name & model storage URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_name = 'blerssi-service'\n",
    "\n",
    "if model_store == 'minio':\n",
    "    storageURI = \"s3://\" + minio_bucket + '/' + TF_EXPORT_DIR\n",
    "    print(storageURI)\n",
    "    \n",
    "elif model_store == 'workspace-vol':\n",
    "    #Retrieve current workspace volume name from Pod specification of current Notebook server \n",
    "    !kubectl get pods $HOSTNAME -o yaml -n anonymous > podspec\n",
    "    with open(\"podspec\") as f:\n",
    "        content = yaml.safe_load(f)\n",
    "        for elm in content['spec']['volumes']:\n",
    "            if 'workspace-' in elm['name']:\n",
    "                pvc = elm['name']\n",
    "    os.remove('podspec')\n",
    "\n",
    "    #Add '/' to TF_MODEL_DIR if not present and storing it in a new variable, \n",
    "    #for the sake of returning successful storageURI    \n",
    "    if TF_MODEL_DIR[0] != '/':     \n",
    "        MOD_TF_MODEL_DIR = '/' + TF_MODEL_DIR\n",
    "    else:\n",
    "        MOD_TF_MODEL_DIR = TF_MODEL_DIR\n",
    "    \n",
    "    #Remove '/' from TF_EXPORT_DIR if present and storing it in a new variable, \n",
    "    #for the sake of returning successful storageURI\n",
    "    if TF_EXPORT_DIR[0] == '/':\n",
    "        MOD_TF_EXPORT_DIR = TF_EXPORT_DIR[1:]\n",
    "    else:\n",
    "        MOD_TF_EXPORT_DIR = TF_EXPORT_DIR\n",
    "\n",
    "    storageURI = \"pvc://\" + pvc + os.path.join(MOD_TF_MODEL_DIR, 'export/', MOD_TF_EXPORT_DIR)\n",
    "    print(storageURI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define configuration for inference service creation\n",
    "\n",
    "Define configuration for inference service for the respective model stores, and write to .yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_store == 'minio':\n",
    "    \n",
    "    minio_blerssi_kf = f\"\"\"apiVersion: \"serving.kubeflow.org/v1alpha2\"\n",
    "kind: \"InferenceService\"\n",
    "metadata:\n",
    "  name: {svc_name}\n",
    "  namespace: anonymous\n",
    "spec:\n",
    "  default:\n",
    "    predictor:\n",
    "      serviceAccountName: minio-sa\n",
    "      tensorflow:\n",
    "        storageUri: {storageURI}\n",
    "\"\"\"\n",
    "    \n",
    "    kfserving = yaml.safe_load(minio_blerssi_kf)\n",
    "    with open('blerssi-kfserving.yaml', 'w') as file:\n",
    "        yaml_kfserving = yaml.dump(kfserving,file)\n",
    "        \n",
    "    ! cat blerssi-kfserving.yaml\n",
    "    \n",
    "elif model_store == 'workspace-vol':\n",
    "    \n",
    "    wsvol_blerssi_kf = f\"\"\"apiVersion: \"serving.kubeflow.org/v1alpha2\"\n",
    "kind: \"InferenceService\"\n",
    "metadata:\n",
    "  name: {svc_name}\n",
    "  namespace: anonymous\n",
    "spec:\n",
    "  default:\n",
    "    predictor:\n",
    "      tensorflow:\n",
    "        storageUri: {storageURI}\n",
    "\"\"\"\n",
    "    \n",
    "    kfserving = yaml.safe_load(wsvol_blerssi_kf)\n",
    "    with open('blerssi-kfserving.yaml', 'w') as file:\n",
    "        yaml_kfserving = yaml.dump(kfserving,file)\n",
    "        \n",
    "    ! cat blerssi-kfserving.yaml    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the configuration .yaml file\n",
    "\n",
    "By applying the configuration .yaml file, serving of BLERSSI model is done using Kubeflow KFServing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl apply -f blerssi-kfserving.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check whether inferenceservice is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get inferenceservice -n anonymous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "Wait for inference service READY=\"True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_csv(os.path.join(path, 'iBeacon_RSSI_Labeled.csv')) #Labeled dataset\n",
    "# Input Data Preprocessing \n",
    "df_full = df_full.drop(['date'],axis = 1)\n",
    "df_full = df_full.drop(['location'],axis = 1)\n",
    "df_full[FEATURES] = (df_full[FEATURES])/(-200)\n",
    "print(df_full.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict data from serving after setting INGRESS_IP\n",
    "\n",
    "Note - Use one of preprocessed row values from previous cell as values for \"instances\" in the below request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v -H \"Host: blerssi-service.anonymous.example.com\" http://INGRESS_IP:31380/v1/models/blerssi-service:predict -d '{\"signature_name\":\"predict\",\"instances\":[{\"b3001\":[-0.458086] , \"b3002\":[-0.6244] , \"b3003\":[2.354243], \"b3004\":[-0.404581] , \"b3005\":[1.421444] , \"b3006\":[1.767642] , \"b3007\":[2.637829] , \"b3008\":[-0.603085] , \"b3009\":[0.382779] , \"b3010\":[-0.378999] , \"b3011\":[-0.341798] , \"b3012\":[-0.303249] , \"b3013\":[-0.327776]}]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup after prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete KFserving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete -f blerssi-kfserving.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_store == 'minio':\n",
    "    \n",
    "    #Delete minIo secret\n",
    "    !kubectl delete -f minio-secret.yaml\n",
    "    \n",
    "    #Define minIO service account\n",
    "    !kubectl delete -f minio-serviceaccount.yaml\n",
    "    \n",
    "    #Delete minIO objects & minIO bucket\n",
    "    model_response = minio_uploader.client.list_objects(Bucket=minio_bucket)\n",
    "    \n",
    "    #Delete locally stored model folder\n",
    "    !rm -rf $TF_MODEL_DIR\n",
    "\n",
    "    obj_list = []\n",
    "    for obj_name in model_response['Contents']:\n",
    "        obj_list.append({'Key' : obj_name['Key']})\n",
    "        \n",
    "    print(\"Deleting the stored objects in minIO bucket.....\")\n",
    "    minio_uploader.client.delete_objects(Bucket=minio_bucket, Delete={'Objects' : obj_list})\n",
    "    \n",
    "    print(\"Deleting the minIO bucket.....\")\n",
    "    minio_uploader.client.delete_bucket(Bucket=minio_bucket)\n",
    "    print(\"Done\")\n",
    "    \n",
    "elif model_store == 'workspace-vol':\n",
    "    \n",
    "    #Clean up the model folder\n",
    "    print(\"Cleaning up model folder...\")\n",
    "    !rm -rf $TF_MODEL_DIR\n",
    "    print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
